{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qzniOkIbnDkp",
    "outputId": "104483d7-ec9e-489e-d8e4-170bab449632"
   },
   "outputs": [],
   "source": [
    "!pip install -q streamlit scikit-learn pandas numpy sentence-transformers nltk\n",
    "!npm install -g localtunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZKRsZd9n5_W"
   },
   "outputs": [],
   "source": [
    "preprocess_code = '''\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "CONTRACTIONS = {\n",
    "    \"don't\": \"do not\", \"can't\": \"cannot\", \"won't\": \"will not\",\n",
    "    \"i'm\": \"i am\", \"it's\": \"it is\", \"you're\": \"you are\",\n",
    "    \"they're\": \"they are\", \"we're\": \"we are\", \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "    \"doesn't\": \"does not\", \"didn't\": \"did not\", \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\", \"hadn't\": \"had not\", \"couldn't\": \"could not\",\n",
    "    \"wouldn't\": \"would not\", \"shouldn't\": \"should not\", \"mightn't\": \"might not\",\n",
    "    \"mustn't\": \"must not\"\n",
    "}\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\\\S+|www\\\\S+|https\\\\S+\", \"\", text)\n",
    "    text = re.sub(r\"\\\\S+@\\\\S+\", \"\", text)\n",
    "    for c, f in CONTRACTIONS.items():\n",
    "        text = text.replace(c, f)\n",
    "    text = re.sub(r\"[^a-z\\\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\\\s+\", \" \", text).strip()\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "'''\n",
    "with open(\"preprocessing.py\", \"w\") as f:\n",
    "    f.write(preprocess_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T7OFsKlMn-VM",
    "outputId": "e9cf79d6-f890-4796-cc23-fb9145e7bc59"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from google.colab import files\n",
    "from preprocessing import clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "QsBXyY8DoB6n",
    "outputId": "ae681c9e-79de-41aa-faf3-9e29ed7be643"
   },
   "outputs": [],
   "source": [
    "uploaded = files.upload()\n",
    "import io\n",
    "df = pd.read_csv(io.BytesIO(next(iter(uploaded.values()))), sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H9eoDUbBorot"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "\n",
    "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
    "\n",
    "    contractions = {\n",
    "        \"don't\": \"do not\", \"can't\": \"cannot\", \"won't\": \"will not\",\n",
    "        \"i'm\": \"i am\", \"it's\": \"it is\", \"you're\": \"you are\",\n",
    "        \"they're\": \"they are\", \"we're\": \"we are\", \"isn't\": \"is not\",\n",
    "        \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "        \"doesn't\": \"does not\", \"didn't\": \"did not\", \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\", \"hadn't\": \"had not\", \"couldn't\": \"could not\",\n",
    "        \"wouldn't\": \"would not\", \"shouldn't\": \"should not\", \"mightn't\": \"might not\",\n",
    "        \"mustn't\": \"must not\"\n",
    "    }\n",
    "    for c, f in contractions.items():\n",
    "        text = text.replace(c, f)\n",
    "\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(w) for w in text.split() if w not in stop_words]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df[\"verified_reviews\"] = df[\"verified_reviews\"].apply(clean_text)\n",
    "\n",
    "df = df[df[\"verified_reviews\"].str.strip() != \"\"]\n",
    "\n",
    "df = df[df[\"feedback\"].isin([\"positive\", \"negative\"])]\n",
    "\n",
    "df[\"feedback\"] = df[\"feedback\"].map({\"positive\": 1, \"negative\": 0}).astype(int)\n",
    "y = 1 - df[\"feedback\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "b335544d241f422387ce2092681a333e",
      "0ad0fc9a4e9b4c2581268a43cc622f82",
      "da8ff817d26a4e9981f13fd3f758f3d5",
      "0234a601a45f43ada68f35041ed4fa3e",
      "7f7d77d48af648c6b212443c5d1038ed",
      "66620812f110429cb5281e04bbe42152",
      "56c54482b1954735a963a2a8d30d81d0",
      "45e5c28006ad4f96b4b17413d30480f3",
      "beb56c20a2c7419e9e2e6b8e1a393b0e",
      "f8f05d668c8145c980db86b5269def73",
      "ac1526d74404491ab314201b354057ce"
     ]
    },
    "id": "vYY7BMtAoQNc",
    "outputId": "059adee3-80d3-4861-ed71-9b255e183165"
   },
   "outputs": [],
   "source": [
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_embed = embedder.encode(df[\"verified_reviews\"].tolist(), show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-R24bnxfoTAc",
    "outputId": "3e930bba-4a72-4e7e-c31c-ee3c96ec78e8"
   },
   "outputs": [],
   "source": [
    "print(\"Raw feedback unique values:\", df[\"feedback\"].unique())\n",
    "\n",
    "df[\"feedback\"] = df[\"feedback\"].astype(str).str.lower().str.strip()\n",
    "\n",
    "print(\"Normalized feedback values:\", df[\"feedback\"].unique())\n",
    "\n",
    "positive_keywords = [\"positive\", \"pos\", \"1\", \"yes\", \"true\"]\n",
    "negative_keywords = [\"negative\", \"neg\", \"0\", \"no\", \"false\"]\n",
    "\n",
    "label_map = {}\n",
    "for val in df[\"feedback\"].unique():\n",
    "    if any(k in val for k in positive_keywords):\n",
    "        label_map[val] = 1\n",
    "    elif any(k in val for k in negative_keywords):\n",
    "        label_map[val] = 0\n",
    "\n",
    "print(\"Generated label_map:\", label_map)\n",
    "\n",
    "df[\"feedback\"] = df[\"feedback\"].map(label_map)\n",
    "\n",
    "df = df[df[\"feedback\"].isin([0, 1])]\n",
    "\n",
    "df[\"verified_reviews\"] = df[\"verified_reviews\"].fillna(\"\").apply(clean_text)\n",
    "df = df[df[\"verified_reviews\"].str.strip() != \"\"]\n",
    "\n",
    "y = 1 - df[\"feedback\"]\n",
    "\n",
    "print(f\"Samples remaining after cleaning: {len(df)}\")\n",
    "print(\"Unique feedback values after mapping:\", df[\"feedback\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qrL_u5R7oVMH",
    "outputId": "2859045d-ed48-4cf7-cae4-5b077457a6d5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "for file in [\"model.pkl\", \"embedder.pkl\"]:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "\n",
    "df = pd.read_csv(\"amazon_alexa.tsv\", sep=\"\\t\")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df[\"verified_reviews\"] = df[\"verified_reviews\"].fillna(\"\").apply(clean_text)\n",
    "\n",
    "df = df[df[\"verified_reviews\"].str.strip() != \"\"]\n",
    "\n",
    "y = df[\"feedback\"]\n",
    "\n",
    "model_embed = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_embed = model_embed.encode(df[\"verified_reviews\"].tolist())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_embed, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "model = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "with open(\"model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "with open(\"embedder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_embed, f)\n",
    "\n",
    "with open(\"model.pkl\", \"rb\") as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "with open(\"embedder.pkl\", \"rb\") as f:\n",
    "    model_embed = pickle.load(f)\n",
    "\n",
    "print(\"âœ… Model and embedder saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9xo-eJPFqtbb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for file in [\"model.pkl\", \"embedder.pkl\"]:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "CpkecbkWrR4l",
    "outputId": "09035316-1ad1-4874-93a1-5a9772b8b03c"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "import pandas as pd\n",
    "df_raw = pd.read_csv(\"amazon_alexa.tsv\", sep=\"\\t\")\n",
    "print(df_raw.shape)\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxu112KAuN33",
    "outputId": "c52817b0-0396-4b37-f475-a5570b5e0f41"
   },
   "outputs": [],
   "source": [
    "!curl https://loca.lt/mytunnelpassword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hTGtP-JIocbS",
    "outputId": "2fb810e7-618f-4928-9b89-a771baae2eb0"
   },
   "outputs": [],
   "source": [
    "app_code = '''\n",
    "import streamlit as st\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    return text.strip()\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    with open(\"model.pkl\", \"rb\") as f:\n",
    "        model = pickle.load(f)\n",
    "    with open(\"embedder.pkl\", \"rb\") as f:\n",
    "        embedder = pickle.load(f)\n",
    "    return model, embedder\n",
    "\n",
    "model, embedder = load_model()\n",
    "\n",
    "st.title(\"Amazon Alexa Review Sentiment Analysis\")\n",
    "st.write(\"Predict whether a review is **Positive** or **Negative**.\")\n",
    "\n",
    "user_input = st.text_area(\"Enter your review:\")\n",
    "\n",
    "if st.button(\"Predict\"):\n",
    "    cleaned = clean_text(user_input)\n",
    "    if cleaned == \"\":\n",
    "        st.warning(\"Please enter valid text.\")\n",
    "    else:\n",
    "        emb = embedder.encode([cleaned])\n",
    "        pred = model.predict(emb)\n",
    "        sentiment = \"Positive\" if pred[0] == 1 else \"Negative\"\n",
    "        st.success(f\"Prediction: **{sentiment}**\")\n",
    "'''\n",
    "\n",
    "with open(\"app.py\", \"w\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "!nohup streamlit run app.py --server.port 8501 &>/tmp/logs.txt &\n",
    "\n",
    "!npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
